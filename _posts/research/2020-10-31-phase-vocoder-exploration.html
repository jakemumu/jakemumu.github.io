---
layout: research_post
company: Research
cover_image: /assets/images/research/phase_vocoder/cover.png
description: An exploration in hacking together a high quality phase vocoder while avoiding math.
published: false
---

<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.21.0/themes/prism.min.css" rel="stylesheet" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.21.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />

<section class="silver-bg project-cover quarterheight">
  <div class="valign">
    <article class="welcome-heading text-center">
      <h1 class="dark"><span class="font2">Phase Vocoder <br> [In Progress]</span></h1>
    </article>
  </div>
</section>

<section class="sub-wrap add-top add-bottom project-info">
  <div class="container-fluid">
    <div class="row">
      <article class="col-md-4 text-left caption">
        <h2 class="project-title font2 dark">Phase Vocoder</h2>
        <div class="liner color-bg"></div>
        <h3 class="font4 grey remove-top">Fall 2020</h3>
      </article>
      <article class="col-md-8 text-left">
        <h4 class="font2 dark">What we're doing</h4>
        <p class="font4 add-bottom-quarter">
          Over the past few days, I've been reading up on the phase vocoder, as I'd like a higher quality way to transpose audio signals in my music apps, without needing outside SDKs or libraries.
          I've already created many time-domain overlap & add algorithms which have gone into commercial products, but the audible artifacts of the algorithms have always been a creative approach to time/pitch scaling,
          and never a commercially viable solution for transparent time/pitch scaling.
          <br/><br/>
          Usually when I work in the DSP space, my approach is a bit different. It helps me greatly to work up to current state of the art, by first attempting my naive approaches, thus shooting myself in the foot and understanding exactly why the naive approach doesn't work, 
          which is exactly what we're going to do here.
         </p>

         <h4 class="font2 dark">PV / Overlap And Add Overview</h4>
         <p class="font4 add-bottom-quarter">
          As I mentioned, this is going to be a complete coming to terms of understanding the Phase Vocoder, at the time of writing this initial start, I, perhaps like you, do not yet fully understand what it is.
          <br/><br/>
          My naive understanding is that the phase vocoder is basically an implementation of the overlap & add concept in the frequency space, as opposed to the time space.
          <br/><br/>
          If you don't yet understand the overlap and add concept, it is quite simple, requires very little math understanding to implement in the time domain, and is very effective with
          a lot of potential creative effects. This graphic from one of my old research papers demonstrates the concept roughly: 
          <br/><br/>
          <img class="img-responsive" src="/assets/images/research/phase_vocoder/assets/grain_overlap.png"/>
          <br/>
          What this image is attempting to illustrate, is how an overlap and add process in the time domain can be used to decouple pitch and time. 
          The white lines represent the concepts of grains, which is basically, a variable amount of audio data, represented as a window. This simply means,
          you obtain a section of audio, and apply a fade over the section, so both the start and end are silent. What this allows you to do, is jump and scrub
          around a piece of audio, without generating clicks as you move your playhead around to different regions of the sound.
          <br/><br/>
          In this image, we see on the left, that if we consider the entire piece of audio, as a single grain, when you pitch the audio up, it resamples or scrubs through
          the audio faster, generating a change in pitch, but also a change in the length. On the right, we see the grains broken down to be much smaller, now, what we can
          do is space our grain appropriately, so that they retrigger at points, to shift the pitch of the audio up, while distributing them across the sample correct to
          keep the time intact. This is the overlap and add concept, when we resample with this approach, multiple grains or windows of sounds are adding on top of each other
          so that the listener cannot perceive that there isn't a steady stream of audio happening, and in fact, with the correct overlap and window sizes, this can sound, quite
          good. It could be better, and that is where the phase vocoder comes into play.
          </p>

          <h4 class="font2 dark">Starting With Pitch</h4>
          <p class="font4 add-bottom-quarter">
            Let's break our end goal of a real time processing phase vocoder, down into some smaller steps. Now our goal is to decouple pitch and time. 
            After a bit of reading, it seems there are two common ways of doing this.
            <br><br>
            1. We use overlap & add to make a piece of audio longer or shorter, and then resample it to change its pitch. (I'm currently not sure how this is different than the time domain)
            <br><br>
            2. We compute the FFT of the window, and then process the FFT to shift the pitch of the signals inside of it before performing in the inverse FFT.
            <br><br>
            Option one won't work for us because we want to run in near real time (very little latency), so let's get started with option two, figuring out how to modify FFT bins to shift the frequencies of the signal inside of it.
           </p>

          <h4 class="font2 dark">Modifying The Pitch Of A Signal Through FFT</h4>
          <p class="font4 add-bottom-quarter">
          
            A lot is going on to get this set up and running, and I'll post source code later on, but for now, let's set up a 2 window overlap & add, which reconstructs the
            signal with modified FFT windows.
            <br><br>
            I'm going to use an FFT size of 1024, with a hop size of 512. This means every 512 samples we'll generate a new windowed section of audio. The left half of this window,
            will contain the right half of our previous window and these overlapping will give us a constant stream of audio.

            <br><br>
            1. Read Data Into Circular Buffer
            <br>
            2. When enough data for FFT processing is obtained, process the FFT
            <br>
            3. Move the FFT data around to shift the bin data into our new target bins.
            <br>
            4. Perform the inverse FFT, re-window the signal, and send this to the output.
          </p>

          <pre>
            <code class="language-cpp">
              void MKUPhaseVocoder::processSingleSample(float& inSample)
              {
                // Store a sample in our circular buffer
                mCircularBuffer.pushSingleSample(inSample);
                
                // Check if we have enough data for an FFT run
                if (mCircularBuffer.getNumReady() >= mFFTSize) {

                  // Obtain our "newest" FFT block (Avoid this alloc in real life)
                  AudioBuffer<float> block = mCircularBuffer.getNewestData(mFFTSize);
          
                  // Apply a window function to our FFT to avoid leakage
                  mHannWindow.multiplyWithWindowingTable(block.getWritePointer(0), mFFTSize);
            
                  // add some zero padding, JUCE will deliever the FFT data in place interleaved re,im 
                  block.setSize(1, mFFTSize * 2, true);
                            
                  // Perform the FFT
                  mFFT.performRealOnlyForwardTransform(block.getWritePointer(0));
                }
              }
            </code>
          </pre>

          <p>
          <br>
          So now we have our FFT bins as the real and imaginary numbers, we can try shifting their bins around and see how that sounds.
          <br><br>
          First, lets store out current real and imaginary numbers so we can shuffle them around:
          <br><br>
          </p>

          <pre>
            <code class="language-cpp">
              for (int i = 0; i < mFFTSize; i++) {

                // Obtain real and imaginary numbers
                float real = block.getSample(0, i*2);
                float imag = block.getSample(0, i*2+1);
    
                /* Store them to redistribute before inverse transform */
                mReals[i] = real;
                mImaginaries[i] = imag;
              }
            </code>
          </pre>

          <p>
            <br>
            Alright -- so the first thing we'd naturally attempt, lets shift the bins around by our pitch factor. It's easy to understand why it might not work
            perfectly, but it's a natural first try, and interesting to hear what it sounds like.
            <br><br>
            What we'll do, is loop through our FFT bins, and remap the bin by our pitch factor, for now, 0.5 - 2.0, or one octave up and down.
            <br><br>
          </p>

          <pre>
            <code class="language-cpp">
              // clear the original FFT data, we no longer need it.
              block.clear();
        
              for (int i = 0; i < mFFTSize; i++)
              {
                // map our old bin to our new bin based on our bitch factor
                int new_index = i * pitch;
    
                // check for nyquist
                if (new_index < mFFTSize)
                {
                  // re populate our FFT buffer with our shifted values before performing inverse transform
                  block.setSample(0, new_index * 2, mReals[i]);
                  block.setSample(0, new_index * 2 + 1, mImaginaries[i]);
                }
              }
            </code>
          </pre>

          <p>
            <br>
            The last step is then obviously to get some playback running on these:
            <br><br>
          </p>

          <pre>
            <code class="language-cpp">
            {...
              // Perform the inverse FFT
              mFFT.performRealOnlyInverseTransform(block.getWritePointer(0));

              // Dump the extra padding that was there for imaginary numbers
              block.setSize(1, mFFTSize, true);

              // Find a free output grain to manage synchronizing this buffer (We have 2)
              for (int i = 0; i < mOLAPOutputBuffers.size(); i++) {
                  if (mOLAPOutputBuffers[i]->isRunning() == false) {
                      mOLAPOutputBuffers[i]->initalize(block);
                      break;
                  }
              }
              
              // Tell our circular buffer, how much we consumed (this will control how often we get FFT callbacks)
              mCircularBuffer.setNumRead(mHopSize);
            }
            
            // Now we've exited the FFT processing section and are back in per sample land
            inSample = 0;
            
            // Loop through our "OLAP" grains & sum em up
            for (int i = 0; i < mOLAPOutputBuffers.size(); i++) {
                inSample += mOLAPOutputBuffers[i]->getNextSample();
            }
            </code>
          </pre>

          <p>
            <br>
            So with all of that done, we've got our naive brute force pitch shifting not phase vocoder.
            <br><br>
            Here are the results:
            <br><br>

            Pitch Factor 1: <br>
            <audio controls>
              <source src="/assets/audio/phase_vocoder/naive_pitch_shift/pitch_factor_1.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>

            <br><br>

            Pitch Factor 1.25: <br>
            <audio controls>
              <source src="/assets/audio/phase_vocoder/naive_pitch_shift/pitch_factor_1.25.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>

            <br><br>

            Pitch Factor 1.5: <br>
            <audio controls>
              <source src="/assets/audio/phase_vocoder/naive_pitch_shift/pitch_factor_1.5.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>
          </p>

          <br> <br>
          <img class="img-responsive" src="https://media.giphy.com/media/Y54bNi0kU0oj6/giphy.gif"/>
          <br> <br>

          <h4 class="font2 dark">Naive Review</h4>
          <p class="font4 add-bottom-quarter">
          
            We can hear that this method isn't sounding quite right... And there are a couple of potential issues.
            <br><br>
            1. The naive pitch-shifting approach isn't acceptable
            <br><br>
            2. There isn't enough overlap and add to keep the signal sounding constant. I attempted to increase the overlap of the windows, however this made the results utterly unacceptable and bad sounding. Let's explore why that is.
          </p>

          <h4 class="font2 dark">How To Correctly Pitch Shift An FFT</h4>
          <p class="font4 add-bottom-quarter">
            So now that we've tried our naive pitch-shifting approach, we can look into some real solutions and learn why this isn't so easy. The number one article which keeps coming up over and over
            is <a target="_blank" href="http://blogs.zynaptiq.com/bernsee/pitch-shifting-using-the-ft/">this</a> article from the team at <a target="_blank" href="https://www.zynaptiq.com"> Zynaptiq</a> who have been making awesome audio effects since I was in diapers.
            <br><br>
            The basic summary is this: FFTs are wonderful tools and can give us perfect forward & inverse transforms of a signal, however modifying signals in the FFT space isn't so
            simple as shifting some bins around. The reason for this is what they refer to as "Frequency Resolution" -- and they have some awesome graphics I recommend checking out.
            <br><br>
            The main point is that our STFTs have bins at fixed frequency internals, and unless a frequency perfectly matches up with one of these bins, then the power of the spectra
            around that point, are going to mix around various bins in the FFT. So basically, when you take one bins re,im & pop it into another bin, it's not correctly shifting the
            signals around that frequency
            <br><br>
            The amazing thing, bless these people, is they're super smart and have a solution they've provided us we'll disect together. The claim is that by analyzing changes in the a bins phase over time, we're then able to
            detect the "true" frequency within that bin.
          </p>

          <h4 class="font2 dark"> Correcting Our Approach (Selective Peak Shifting) </h4>
          <p class="font4 add-bottom-quarter">
            So I finally decided to dive in and read more about the actual phase vocoder. <a target="_blank" href="https://www2.spsc.tugraz.at/www-archive/downloads/Bachelor%20Thesis%20Gruenwald.pdf">This</a> paper has been really useful
            and given me some further nomenclature to understand the theory and implementations described in the Zynaptiq article. It also provides us with this beautiful graphic
            showing the phase vocoders structure:
          </p>
          <img class="img-responsive" src="/assets/images/research/phase_vocoder/assets/phase_vocoder_structure.png"/>
          <p class="font4 add-bottom-quarter">
            The way they describe the steps of the phase vocoder in the paper is a 3 step process.

            <br><br>
            1. Analysis
            <br>
            2. Processing
            <br>
            3. Synthesis
            <br><br>

            We're pretty good on analysis and synthesis, the structure of our algorithm allows us to flexibly define the FFT size, the hop size, and all the pieces to deconstruct and
            reconstruct the signal, what we need to do is improve our processing step to use some of the actual techniques of the phase vocoder.
            <br><br>
            DISCLAIMER -- THIS FUNCTIONALITY IS TAKEN FROM THE ZYNAPTIQ SOURCE CODE <a target="_blank" href="http://blogs.zynaptiq.com/bernsee/repo/smbPitchShift.cpp">HERE</a> I TAKE NO CLAIM TO THE FORTHCOMING CODE
            <br><br>
            Step 1: Convert our FFT information into the polar coordinate space so we can use our phase history to approximate the true frequency of the FFT bin.
            <br><br>
            Step 2: Compare our previous phase with our current phase, and what we would expect the change in phase to be given the frequency was exactly centered in the FFT
            <br><br>
            Step 3: Use this information to construct an array of our "true" frequency & magnitudes
          </p>

          <pre>
            <code class="language-cpp">
              for (int i = 0; i < mFFTSize; i++) {
                // Convert the FFT data to a complex
                std::complex<double> complex;
                complex.real(block.getSample(0, i*2));
                complex.imag(block.getSample(0, i*2+1));
    
                // Obtain the magnitude and phase information in polar coordinates
                double magnitude = std::abs(complex);
                double phase = std::arg(complex);
    
                /* compute phase difference */
                float true_frequency = phase - mPhaseHistory[i];
                mPhaseHistory[i] = phase;
    
                /* subtract expected phase difference */
                true_frequency -= (double)i*expected_phase_change_base;
    
                /* map delta phase into +/- Pi interval */
                true_frequency = constrainAngle(true_frequency);
    
                /* get deviation from bin frequency from the +/- Pi interval */
                true_frequency = mOverlapRate * true_frequency / (2.*M_PI);
                
                /* compute the partials' true frequency */
                true_frequency = (double)i * frequency_per_bin + true_frequency * frequency_per_bin;
    
                /* store magnitude and true frequency in analysis arrays */
                mMagnitudes[i] = magnitude;
                mFrequencies[i] = true_frequency;
              }
            </code>
          </pre>

          <p>
            <br>
            So what this gives us is interesting. What I've done is passed a 440 hz sine tone through the algorithm and printed out the results. 
            <br><br>
            The final result is an array equal to our FFT size, however, instead of a direct mapping to the frquency of the bin according to our FFT size,
            we're given an array with the "true" frequency of the data contained inside of the FFT, as well as a magnitude for that bin. Essentially, all of the bins
            around our bin containing 440 hz, is a bit of a bell into a peak at that bin, where the bins surrounding it have had their frequency replaced with the true 440 hz frequency, 
            with less magnitude, which is compensating for out frequency leakage/smear across the bins.
            <br><br>
            Here is a small subset of the data:
          </p>
          <br>

          <pre>
            <code class="language-cpp">
              ---
              Bin: 3
              Bin Center Hz: 140.625
              Bin True Frequency: 456.245
              Bin Magnitude: 0.259497
              ---
              Bin: 4
              Bin Center Hz: 187.5
              Bin True Frequency: 447.296
              Bin Magnitude: 0.473034
              ---
              Bin: 5
              Bin Center Hz: 234.375
              Bin True Frequency: 443.04
              Bin Magnitude: 0.927187
              ---
              Bin: 6
              Bin Center Hz: 281.25
              Bin True Frequency: 441.086
              Bin Magnitude: 2.13173
              ---
              Bin: 7
              Bin Center Hz: 328.125
              Bin True Frequency: 440.282
              Bin Magnitude: 6.80137
              ---
              Bin: 8
              Bin Center Hz: 375
              Bin True Frequency: 440.027
              Bin Magnitude: 59.8779
              ---
              Bin: 9
              Bin Center Hz: 421.875
              Bin True Frequency: 439.994
              Bin Magnitude: 232.307
              ---
              Bin: 10
              Bin Center Hz: 468.75
              Bin True Frequency: 440.005
              Bin Magnitude: 199.708
              ---
              Bin: 11
              Bin Center Hz: 515.625
              Bin True Frequency: 439.966
              Bin Magnitude: 29.7208
              ---
              Bin: 12
              Bin Center Hz: 562.5
              Bin True Frequency: 439.83
              Bin Magnitude: 5.05218
              ---
              Bin: 13
              Bin Center Hz: 609.375
              Bin True Frequency: 439.578
              Bin Magnitude: 1.77241
              ---
              Bin: 14
              Bin Center Hz: 656.25
              Bin True Frequency: 439.21
              Bin Magnitude: 0.828722
              ---
              Bin: 15
              Bin Center Hz: 703.125
              Bin True Frequency: 438.731
              Bin Magnitude: 0.455199
              ---
              Bin: 16
              Bin Center Hz: 750
              Bin True Frequency: 438.153
              Bin Magnitude: 0.277541
              ---
              Bin: 17
              Bin Center Hz: 796.875
              Bin True Frequency: 437.49
              Bin Magnitude: 0.182136
              ---
            </code>
          </pre>

          <p>
            <br>
            What you'll notice, is that there isn't any bin which represents 440hz. So the nearest bins (420 & 468) have had their frequency remapped to the 440
            <br><br>
            So now, we do our same "bin remapping"
            <br><br>
            Primarily, we're reshifting our bins to our new position, along with true pitch we'd expect to see in that bin:
            <br><br>
          </p>

          <pre>
            <code class="language-cpp">
              for (int i = 0; i < mFFTSize; i++) {
                int new_index = i * pitch;
                if (new_index < mFFTSize) {
                    outputMags[new_index] += mMagnitudes[i];
                    outputFreqs[new_index] = mFrequencies[i] * pitch;
                }
              }
            </code>
          </pre>

          <p>
            <br>
            Now that we have our correct FFT data we can synthesize:
            <br><br>
          </p>

          <pre>
            <code class="language-cpp">

              // Clear our block to rebuild it for inverse FFT
              block.clear();

              // RESYNTHESIZE OUTPUT WITH NEW DATA
              for (int i = 0; i < mFFTSize; i++)
              {
                /* get magnitude and true frequency from synthesis arrays */
                float magn = outputMags[i];
                float new_frequency = outputFreqs[i];
    
                /* subtract bin mid frequency */
                new_frequency -= (double)i*frequency_per_bin;
    
                /* get bin deviation from freq deviation */
                new_frequency /= frequency_per_bin;
    
                /* take overlap rate into account */
                new_frequency = 2.*M_PI*new_frequency/mOverlapRate;
    
                /* add the overlap phase advance back in */
                new_frequency += (double)i*expected_phase_change_base;
    
                /* accumulate delta phase to get bin phase */
                mPhaseAccumulator[i] += new_frequency;
                float phase = mPhaseAccumulator[i];
    
                /* get real and imag part and re-interleave */
                block.setSample(0, i * 2, magn*cos(phase));
                block.setSample(0, i * 2 + 1, magn*sin(phase));
              }
      
              /* now invert and store for output */
              mFFT.performRealOnlyInverseTransform(block.getWritePointer(0));

            </code>
          </pre>

          <p>
            <br>
            So with all of that done, we've got our improved FFT based pitch shifting which is using our phase history to
            more accurately shift the pitches. I've also gone ahead and cranked our overlap rate up since this new method can
            support a much higher overlap.
            <br><br>
            Here are the results:
            <br><br>

            Original: <br>
            <audio controls>
              <source src="/assets/audio/phase_vocoder/proper_peak_detection/Original.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>

            <br><br>

            Ableton's Complex Pro w/ Pitch Factor 2: <br>
            <audio controls>
              <source src="/assets/audio/phase_vocoder/proper_peak_detection/Ableton.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>

            <br><br>

            Our's Pitch Factor 2: <br>
            <audio controls>
              <source src="/assets/audio/phase_vocoder/proper_peak_detection/MKU.wav" type="audio/wav">
              Your browser does not support the audio element.
            </audio>
          </p>

          <br> <br>
          <img class="img-responsive" src="https://media.giphy.com/media/l4JySAWfMaY7w88sU/giphy.gif"/>
          <br> <br>

          <p>
            So with all of that, we can hear that we've at least entered the ballpark of high-quality pitching shifting algorithms on the market. 
            <br><br>
            For now, I'm going to pause writing this article here and take a break, and dive a bit deeper into understanding where the implementation from
            Zynaptiq comes from, and how we can continue to improve our pitching engine. 
            <br><br>
            Until then thanks for reading (11/1/2020)
          </p>



      </article>
    </div>
  </div>
</section>

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.21.0/components/prism-core.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.21.0/plugins/autoloader/prism-autoloader.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.21.0/plugins/line-numbers/prism-line-numbers.min.js"></script>